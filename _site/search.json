[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "inclass/inclass03/inclass03.html",
    "href": "inclass/inclass03/inclass03.html",
    "title": "in-class Exercise 3",
    "section": "",
    "text": "Installing and loading R packages\nTwo packages will be installed and loaded\nThey are ggraph and tidyverse\n\npacman::p_load(tidyverse,ggraph)\n\nimport data\n\nexam_data <- read.csv(\"data/Exam_data.csv\")\n\n\nggplot(data= exam_data,\n          aes(x = MATHS))+\n    geom_dotplot(dotsize = 0.5)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html",
    "href": "inclass/inclass05/inclass05.html",
    "title": "inclass05",
    "section": "",
    "text": "pacman::p_load(corrplot, tidyverse, ggstatsplot)\n\n\nwine <- read_csv(\"data/wine_quality.csv\")\nview(wine)\npairs(wine[,2:12])\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\npairs(wine[,2:12],lower.panel = NULL)\n\n\n\n\n\nwine.cor <- cor(wine[, 1:11])\ncorrplot(wine.cor)\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p < 0.05\"\n)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#installing-and-launching-r-packages",
    "href": "inclass/inclass05/inclass05.html#installing-and-launching-r-packages",
    "title": "inclass05",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\n\npacman::p_load(ggtern, plotly, tidyverse)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#data-preparation",
    "href": "inclass/inclass05/inclass05.html#data-preparation",
    "title": "inclass05",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nImporting Data\n\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\nPreparing the Data\n\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)\n\n\n\nPlotting Ternary Diagram with R\n\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#datainstall-and-launching-r-packages",
    "href": "inclass/inclass05/inclass05.html#datainstall-and-launching-r-packages",
    "title": "inclass05",
    "section": "DataInstall and Launching R Packages",
    "text": "DataInstall and Launching R Packages\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#importing-and-preparing-the-data-set",
    "href": "inclass/inclass05/inclass05.html#importing-and-preparing-the-data-set",
    "title": "inclass05",
    "section": "Importing and Preparing The Data Set",
    "text": "Importing and Preparing The Data Set\n\nwh <- read_csv(\"data/WHData-2018.csv\")\n\n\nPreparing the data\n\nrow.names(wh) <- wh$Country\n\n\n\nTransforming the data frame into a matrix\n\n\n\nwh1 <- dplyr::select(wh, c(3, 7:12))\nwh_matrix <- data.matrix(wh)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#static-heatmap",
    "href": "inclass/inclass05/inclass05.html#static-heatmap",
    "title": "inclass05",
    "section": "Static Heatmap",
    "text": "Static Heatmap\n\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\nwh_heatmap <- heatmap(wh_matrix)\n\n\n\n\n\nwh_heatmap <- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#creating-interactive-heatmap",
    "href": "inclass/inclass05/inclass05.html#creating-interactive-heatmap",
    "title": "inclass05",
    "section": "Creating Interactive Heatmap",
    "text": "Creating Interactive Heatmap\n\nview(mtcars)\nheatmaply(mtcars)\n\n\n\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#data-trasformation",
    "href": "inclass/inclass05/inclass05.html#data-trasformation",
    "title": "inclass05",
    "section": "Data trasformation",
    "text": "Data trasformation\n\nScaling method\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\nNormalising method\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\nPercentising method\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#clustering-algorithm",
    "href": "inclass/inclass05/inclass05.html#clustering-algorithm",
    "title": "inclass05",
    "section": "Clustering algorithm",
    "text": "Clustering algorithm\n\nManual approach\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nwh_d <- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\n\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#seriation",
    "href": "inclass/inclass05/inclass05.html#seriation",
    "title": "inclass05",
    "section": "Seriation",
    "text": "Seriation\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")"
  },
  {
    "objectID": "inclass/inclass05/inclass05.html#working-with-colour-palettes",
    "href": "inclass/inclass05/inclass05.html#working-with-colour-palettes",
    "title": "inclass05",
    "section": "Working with colour palettes",
    "text": "Working with colour palettes\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\nThe finishing touch\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "inclass/inclass07/inclass07.html",
    "href": "inclass/inclass07/inclass07.html",
    "title": "inclass07",
    "section": "",
    "text": "pacman::p_load(sf,tmap,tidyverse) #simple feature\n\nGeospatial Data Wrangling\n\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nRows: 306 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): NAME, ADDRESS, OUTLET TYPE\ndbl (4): POSTCODE, XCOORD, YCOORD, Gp1Gp2 Winnings\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlist(sgpools)\n\n[[1]]\n# A tibble: 306 × 7\n   NAME                            ADDRESS POSTC…¹ XCOORD YCOORD OUTLE…² Gp1Gp…³\n   <chr>                           <chr>     <dbl>  <dbl>  <dbl> <chr>     <dbl>\n 1 Livewire (Marina Bay Sands)     2 Bayf…   18972 30842. 29599. Branch        5\n 2 Livewire (Resorts World Sentos… 26 Sen…   98138 26704. 26526. Branch       11\n 3 SportsBuzz (Kranji)             Lotus …  738078 20118. 44888. Branch        0\n 4 SportsBuzz (PoMo)               1 Sele…  188306 29777. 31382. Branch       44\n 5 Prime Serangoon North           Blk 54…  552542 32239. 39519. Branch        0\n 6 Singapore Pools Woodlands Cent… 1A Woo…  731001 21012. 46987. Branch        3\n 7 Singapore Pools 64 Circuit Rd … Blk 64…  370064 33990. 34356. Branch       17\n 8 Singapore Pools 88 Circuit Rd … Blk 88…  370088 33847. 33976. Branch       16\n 9 Singapore Pools Anchorvale Rd … Blk 30…  540308 33910. 41275. Branch       21\n10 Singapore Pools Ang Mo Kio N2 … Blk 20…  560202 29246. 38943. Branch       25\n# … with 296 more rows, and abbreviated variable names ¹​POSTCODE,\n#   ²​`OUTLET TYPE`, ³​`Gp1Gp2 Winnings`"
  },
  {
    "objectID": "inclass/inclass08/Untitled.html",
    "href": "inclass/inclass08/Untitled.html",
    "title": "inclass08",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\nData Wrangling"
  },
  {
    "objectID": "inclass/inclass08/Untitled.html#creating-facet-graphs",
    "href": "inclass/inclass08/Untitled.html#creating-facet-graphs",
    "title": "inclass08",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\n\nWorking with facet_edges()\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\nWorking with facet_edges()\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\nA framed facet graph\n\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\nWorking with facet_nodes()\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "inclass/inclass08/Untitled.html#network-metrics-analysis",
    "href": "inclass/inclass08/Untitled.html#network-metrics-analysis",
    "title": "inclass08",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nVisualising network metrics\n\n\n\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\nVisualising Community\n\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `community = as.factor(group_edge_betweenness(weights = Weight,\n  directed = TRUE))`.\nCaused by warning in `cluster_edge_betweenness()`:\n! At core/community/edge_betweenness.c:493 : Membership vector will be selected based on the highest modularity score.\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ng + theme_graph()"
  },
  {
    "objectID": "inclass/inclass06/Hands-on_Ex06-VisTime.html",
    "href": "inclass/inclass06/Hands-on_Ex06-VisTime.html",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a horizon chart"
  },
  {
    "objectID": "inclass/inclass06/Hands-on_Ex06-VisTime.html#getting-started",
    "href": "inclass/inclass06/Hands-on_Ex06-VisTime.html#getting-started",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Getting Started",
    "text": "Getting Started\n::: callout-info ## Do It Yourself Write a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\n\nShow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, tidyverse, readxl, knitr, data.table)"
  },
  {
    "objectID": "inclass/inclass06/Hands-on_Ex06-VisTime.html#plotting-calendar-heatmap",
    "href": "inclass/inclass06/Hands-on_Ex06-VisTime.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Plotting Calendar Heatmap",
    "text": "Plotting Calendar Heatmap\nIn this section, you will learn how to plot a calender heatmap programmetically by using ggplot2 package.\n\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\nThe Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\nImporting the data\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\nExamining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n\ntz field stores time zone of the source IP address.\n\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\nData Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nNote: ymd_hms() and hour() are from lubridate package and weekdays() is a base R function.\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nNote: Beside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\nBuilding the Calendar Heatmaps\n\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\nThings to learn from the code chunk: - a tibble data table called grouped is derived by aggregating the attack by wkday and hour fields. - a new field called n is derived by using group_by() and count() functions. - na.omit() is used to exclude missing value. - geom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles. - theme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot. - coord_equal() is used to ensure the plot will have an aspect ratio of 1:1. - scale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\nBuilding Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "inclass/inclass06/Hands-on_Ex06-VisTime.html#cycle-plot",
    "href": "inclass/inclass06/Hands-on_Ex06-VisTime.html#cycle-plot",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Cycle Plot",
    "text": "Cycle Plot\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nData Preparation\n\nStep 1: Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\nStep 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\nStep 4: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\nStep 5: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nPlotting the cycle plot\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME01/takehome01.html",
    "href": "TAKEHOME/TAKEHOME01/takehome01.html",
    "title": "Takehome01",
    "section": "",
    "text": "](images/image-400834270.png)\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME02/takehome2.html#critique",
    "href": "TAKEHOME/TAKEHOME02/takehome2.html#critique",
    "title": "Takehome02",
    "section": "Critique",
    "text": "Critique\n\nclarity\n-Title is not clear enough.from the title, users cannot tell what topic and info will be delivered.a possible title could be\nAge sex pyramid in Singapore: Top 9 most populated planning area - June 2022\n-Bins plotted too tightly.there are too many bins from the whole view, it is hard for viewers to locate and identify a apecific figure and pattern.\n-Lack of indicators. there are not any reference line to help viewers to locate any detail of the figure\n\n\nAesthetics\n-Do not have any color combination. there are only blue and red with white background which can not attract any attention\n-Too little area between axis and graph. There is currently no segreation between the graph area and the axis. try to add color and other graph to make it easier for viewers to see"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME02/takehome2.html#libraries",
    "href": "TAKEHOME/TAKEHOME02/takehome2.html#libraries",
    "title": "Takehome02",
    "section": "Libraries",
    "text": "Libraries\nThe R packages we’ll use for this analysis are:\n\ntidyverse - a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\nggplot2 - a system for declaratively creating graphics, based on The Grammar of Graphics (ggplot2 is included in the tidyverse package, i’m highlighting it here for emphasis, since it’s our main tool for visualisation)\nggthemes - The ggthemes package provides extra themes, geoms, and scales for the ggplot2 package\nggiraph - a package that provides interactive elements to ggplot like animations and tooltips (was not used after experimenting with it, leaving it here for reference)\nplotly - another package that provides interactive elements to ggplot (was not used after experimenting with it, leaving it here for reference)"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME02/takehome2.html#preparing-the-data-set",
    "href": "TAKEHOME/TAKEHOME02/takehome2.html#preparing-the-data-set",
    "title": "Takehome02",
    "section": "Preparing the Data Set",
    "text": "Preparing the Data Set\n\nloading Packages\n\npacman::p_load(tidyverse, ggthemes, ggiraph, plotly)\n\n\n\nImporting and tidying the data\n\nsg <- read_csv('data/respopagesexfa2022.csv')\n\nAfter importing the original data into Rstudio, i want to check if there is any incorrect or messy\n\nstr(sg)\n\nspc_tbl_ [75,696 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA  : chr [1:75696] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr [1:75696] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG  : chr [1:75696] \"0_to_4\" \"0_to_4\" \"0_to_4\" \"0_to_4\" ...\n $ Sex : chr [1:75696] \"Males\" \"Males\" \"Males\" \"Males\" ...\n $ FA  : chr [1:75696] \"<= 60\" \">60 to 80\" \">80 to 100\" \">100 to 120\" ...\n $ Pop : num [1:75696] 0 10 20 60 10 0 0 0 20 50 ...\n $ Time: num [1:75696] 2022 2022 2022 2022 2022 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Sex = col_character(),\n  ..   FA = col_character(),\n  ..   Pop = col_double(),\n  ..   Time = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nWe can observe that there are more data than we really need, so i choose select function to select some columns.\n\nsgsubset <- sg %>% \n  select(PA, AG, Sex, Pop)\nnames(sgsubset) <-c('Planning_Area', 'Age_group', 'Gender', 'Population')\n\nUse level to check the order of factors\n\nlevels(factor(sgsubset$Age_group))\n\n [1] \"0_to_4\"      \"10_to_14\"    \"15_to_19\"    \"20_to_24\"    \"25_to_29\"   \n [6] \"30_to_34\"    \"35_to_39\"    \"40_to_44\"    \"45_to_49\"    \"5_to_9\"     \n[11] \"50_to_54\"    \"55_to_59\"    \"60_to_64\"    \"65_to_69\"    \"70_to_74\"   \n[16] \"75_to_79\"    \"80_to_84\"    \"85_to_89\"    \"90_and_over\"\n\n\nI notice that “5_to_9” is out of place. Use mutate() and arrange() to correct this.\n\norder <- c(\"0_to_4\", \"5_to_9\", \"10_to_14\", \"15_to_19\", \"20_to_24\", \"25_to_29\", \"30_to_34\", \"35_to_39\", \"40_to_44\", \"45_to_49\", \"50_to_54\", \"55_to_59\", \"60_to_64\", \"65_to_69\", \"70_to_74\", \"75_to_79\", \"80_to_84\", \"85_to_89\", \"90_and_over\")\n\nsgsubset <- sgsubset %>%\n  mutate(Age_group =  factor(Age_group, levels = order)) %>%\n  arrange(Age_group)\n\nlevels(sgsubset$Age_group)\n\n [1] \"0_to_4\"      \"5_to_9\"      \"10_to_14\"    \"15_to_19\"    \"20_to_24\"   \n [6] \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"    \"45_to_49\"   \n[11] \"50_to_54\"    \"55_to_59\"    \"60_to_64\"    \"65_to_69\"    \"70_to_74\"   \n[16] \"75_to_79\"    \"80_to_84\"    \"85_to_89\"    \"90_and_over\"\n\n\n\n\nPreparation for visualisation\n\nCheck_PA <- sgsubset %>%\n  group_by(`Planning_Area`) %>%\n  summarise(sum_pop = sum(Population), .groups = 'drop') %>%\n  arrange(sum_pop,.by_group = TRUE) %>%\n  top_n(9) %>%\n  ungroup()\n\nCheck_PA\n\n# A tibble: 9 × 2\n  Planning_Area sum_pop\n  <chr>           <dbl>\n1 Punggol        186250\n2 Choa Chu Kang  190460\n3 Yishun         222770\n4 Hougang        227720\n5 Woodlands      252720\n6 Sengkang       253050\n7 Jurong West    258520\n8 Tampines       265610\n9 Bedok          278870\n\n\n\nsgsubsettop9 <- sgsubset %>% filter(`Planning_Area` %in% c('Bedok', 'Tampines', 'Jurong West', 'Sengkang', 'Woodlands', 'Hougang', 'Yishun', 'Choa Chu Kang', 'Punggol'))\n\n\n\nVisualising the Age-Sex Pyramid in a Trellis Display\nDesign X and Y\n\nagesexP <- ggplot(sgsubsettop9,aes(x = `Age_group`, y = Population,fill = Gender)) + \n  geom_bar(data = subset(sgsubsettop9,Gender == 'Females'), stat = 'identity') + \n  geom_bar(data = subset(sgsubsettop9,Gender == 'Males'), stat = 'identity', mapping = aes(y = -(Population))) + \n  coord_flip() + \n  facet_wrap(~`Planning_Area`,ncol = 3)\n\ndesign\n\nagesexP +\n  ggtitle(\"Singapore Age-Sex Pyramid (Age sex pyramid in Singapore: Top 9 most populated planning area - June 2022\") + \n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, vjust = 3)) + \n  xlab(\"Age Group\") + \n  ylab(\"Population\")+\n  scale_fill_manual(values=c('coral','cyan'))"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME02/takehome2.html#learning-points",
    "href": "TAKEHOME/TAKEHOME02/takehome2.html#learning-points",
    "title": "Takehome02",
    "section": "Learning points",
    "text": "Learning points\nTableau and Rstudio are two different mindset visualization methods.\nfirst one is more visual and convenient to create presentation ready visualisations without any coding knowledge. This makes tableau much more accessible to the general public and beginners\nOn the other hand, visualisations in ‘R’ can achieve the same if not better visualisation that Tableau with much more granular control over the various attributes and objects shown. However the difficult part is coding knowledge required.\nIn the end, which tool to be used may depended on the people and the requirement."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME04/TAKEHOME04.html",
    "href": "TAKEHOME/TAKEHOME04/TAKEHOME04.html",
    "title": "TAKEHOME04 Putting Visual Analytics into Practical Use",
    "section": "",
    "text": "In this take-home exercise, we are going to find the impact of COVID-19 towards international trading by using appropriate analytical visualisation techniques learned in Lesson 6: It's About Time. including proper interactive techniques which can be used to enhance user and data discovery experiences."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#data-processing",
    "href": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#data-processing",
    "title": "TAKEHOME04 Putting Visual Analytics into Practical Use",
    "section": "Data Processing",
    "text": "Data Processing\n\n2.1 Transitting into Data Frame: import & export\n\nimports_transposed <- imports %>%\n  select(matches(\"Data|202[0-2]\")) %>%    # select columns containing \"Data\", \"2020\", \"2021\", or \"2022\"\n  t() %>%                                # transpose the data frame\n  row_to_names(row_number = 1) %>% \n  as.data.frame() %>%                    # convert to a data frame\n  tibble::rownames_to_column(\"timestamp\") %>%  # add a timestamp column using row names\n  mutate(timestamp = lubridate::ymd(paste0(timestamp, \"01\"))) %>%   # convert timestamp to date format\n  arrange(timestamp)                     # sort by timestamp in ascending order\n\n\nexports_transposed <- exports %>%\n  select(matches(\"Data|202[0-2]\")) %>%    # select columns containing \"Data\", \"2020\", \"2021\", or \"2022\"\n  t() %>%                                # transpose the data frame\n  row_to_names(row_number = 1) %>% \n  as.data.frame() %>%                    # convert to a data frame\n  tibble::rownames_to_column(\"timestamp\") %>%  # add a timestamp column using row names\n  mutate(timestamp = lubridate::ymd(paste0(timestamp, \"01\"))) %>%   # convert timestamp to date format\n  arrange(timestamp)                     # sort by timestamp in ascending order\n\n\nkable(head(imports_transposed))\n\n\n\n \n  \n    timestamp \n    Total Merchandise Imports (Thousand Dollars) \n    America (Million Dollars) \n    Asia (Million Dollars) \n    Europe (Million Dollars) \n    Oceania (Million Dollars) \n    Africa (Million Dollars) \n    European Union (Million Dollars) \n    Belgium (Thousand Dollars) \n    Denmark (Thousand Dollars) \n    France (Thousand Dollars) \n    Germany, Federal Republic Of (Thousand Dollars) \n    Greece (Thousand Dollars) \n    Ireland (Thousand Dollars) \n    Italy (Thousand Dollars) \n    Luxembourg (Thousand Dollars) \n    Netherlands (Thousand Dollars) \n    United Kingdom (Thousand Dollars) \n    Portugal (Thousand Dollars) \n    Spain (Thousand Dollars) \n    Austria (Thousand Dollars) \n    Finland (Thousand Dollars) \n    Norway (Thousand Dollars) \n    Sweden (Thousand Dollars) \n    Switzerland (Thousand Dollars) \n    Liechtenstein (Thousand Dollars) \n    Malta (Thousand Dollars) \n    Germany, Democratic Republic Of (Thousand Dollars) \n    Hungary (Thousand Dollars) \n    Poland (Thousand Dollars) \n    Estonia (Thousand Dollars) \n    Latvia (Thousand Dollars) \n    Lithuania (Thousand Dollars) \n    Slovenia (Thousand Dollars) \n    Czech Republic (Thousand Dollars) \n    Slovak Republic (Slovakia) (Thousand Dollars) \n    Brunei Darussalam (Thousand Dollars) \n    Indonesia (Thousand Dollars) \n    Malaysia (Thousand Dollars) \n    Philippines (Thousand Dollars) \n    Thailand (Thousand Dollars) \n    Myanmar (Thousand Dollars) \n    Cambodia (Thousand Dollars) \n    Laos People's Democratic Republic (Thousand Dollars) \n    Vietnam, Socialist Republic Of (Thousand Dollars) \n    Japan (Thousand Dollars) \n    Hong Kong (Thousand Dollars) \n    Republic Of Korea (Thousand Dollars) \n    Taiwan (Thousand Dollars) \n    Macau (Thousand Dollars) \n    Mainland China (Thousand Dollars) \n    Afghanistan (Thousand Dollars) \n    Bangladesh (Thousand Dollars) \n    India (Thousand Dollars) \n    Maldives, Republic Of (Thousand Dollars) \n    Nepal (Thousand Dollars) \n    Pakistan (Thousand Dollars) \n    Sri Lanka (Thousand Dollars) \n    Bahrain (Thousand Dollars) \n    Cyprus (Thousand Dollars) \n    Iran (Islamic Republic Of) (Thousand Dollars) \n    Israel (Thousand Dollars) \n    Jordan (Thousand Dollars) \n    Kuwait (Thousand Dollars) \n    Lebanon (Thousand Dollars) \n    Oman (Thousand Dollars) \n    Qatar (Thousand Dollars) \n    Saudi Arabia (Thousand Dollars) \n    Syrian Arab Republic (Thousand Dollars) \n    United Arab Emirates (Thousand Dollars) \n    Yemen (Thousand Dollars) \n    Yemen Democratic (Thousand Dollars) \n    Canada (Thousand Dollars) \n    Puerto Rico (Thousand Dollars) \n    United States (Thousand Dollars) \n    Argentina (Thousand Dollars) \n    Brazil (Thousand Dollars) \n    Chile (Thousand Dollars) \n    Colombia (Thousand Dollars) \n    Ecuador (Thousand Dollars) \n    Mexico (Thousand Dollars) \n    Paraguay (Thousand Dollars) \n    Peru (Thousand Dollars) \n    Uruguay (Thousand Dollars) \n    Venezuela (Thousand Dollars) \n    Netherlands Antilles (Thousand Dollars) \n    Panama (Thousand Dollars) \n    Bahamas (Thousand Dollars) \n    Bermuda (Thousand Dollars) \n    French Guiana (Thousand Dollars) \n    Grenada (Thousand Dollars) \n    Guatemala (Thousand Dollars) \n    Honduras (Thousand Dollars) \n    Jamaica (Thousand Dollars) \n    St Vincent & The Grenadines (Thousand Dollars) \n    Trinidad & Tobago (Thousand Dollars) \n    Anguilla (Thousand Dollars) \n    Other Countries In America (Thousand Dollars) \n    Australia (Thousand Dollars) \n    Fiji (Thousand Dollars) \n    Nauru (Thousand Dollars) \n    New Caledonia (Thousand Dollars) \n    New Zealand (Thousand Dollars) \n    Papua New Guinea (Thousand Dollars) \n    Cocos (Keeling) Islands (Thousand Dollars) \n    French Southern Territories (Thousand Dollars) \n    Norfolk Island (Thousand Dollars) \n    Cook Islands (Thousand Dollars) \n    French Polynesia (Thousand Dollars) \n    Guam (Thousand Dollars) \n    Kiribati (Thousand Dollars) \n    Niue (Thousand Dollars) \n    Solomon Islands (Thousand Dollars) \n    Tuvalu (Thousand Dollars) \n    Wallis & Fatuna Islands (Thousand Dollars) \n    Micronesia (Thousand Dollars) \n    Palau (Thousand Dollars) \n    South Sudan (Thousand Dollars) \n    Other Countries In Oceania (Thousand Dollars) \n    Commonwealth Of Independent States (Thousand Dollars) \n  \n \n\n  \n    2020-01-01 \n    41180224.0 \n    5844.1 \n    27128.1 \n    6859.7 \n    819.7 \n    528.6 \n    4370.9 \n    124193.0 \n    84860.0 \n    1491370.0 \n    1127285.0 \n    10275.0 \n    87483.0 \n    512050.0 \n    9204.0 \n    283975.0 \n    1104098.0 \n    43064.0 \n    122480.0 \n    80100.0 \n    29925.0 \n    58705.0 \n    118465.0 \n    970886.0 \n    2822.0 \n    42930.0 \n    0.0 \n    24246.0 \n    50706.0 \n    4433.0 \n    1331.0 \n    22159.0 \n    4095.0 \n    61739.0 \n    9510.0 \n    278270.0 \n    1736989.0 \n    4979254.0 \n    730334.0 \n    1119938.0 \n    16270.0 \n    17055.0 \n    525.0 \n    381987.0 \n    2138871.0 \n    391417.0 \n    2078134.0 \n    3571356.0 \n    1000.0 \n    5469816.0 \n    7.0 \n    51163.0 \n    818894.0 \n    89.0 \n    1957.0 \n    7491.0 \n    8913.0 \n    654.0 \n    1040.0 \n    335.0 \n    63270.0 \n    2477.0 \n    402328.0 \n    176.0 \n    57540.0 \n    699137.0 \n    669964.0 \n    4.0 \n    816041.0 \n    70.0 \n    0.0 \n    568765.0 \n    8214.0 \n    4497881.0 \n    6161.0 \n    404695.0 \n    7693.0 \n    33246.0 \n    2066.0 \n    276701.0 \n    96.0 \n    3375.0 \n    1205.0 \n    1266.0 \n    32.0 \n    691.0 \n    0.0 \n    81.0 \n    7.0 \n    5.0 \n    1663.0 \n    249.0 \n    94.0 \n    3.0 \n    26.0 \n    3.0 \n    29935.0 \n    695463.0 \n    265.0 \n    4.0 \n    30.0 \n    121022.0 \n    1883.0 \n    113.0 \n    0.0 \n    89.0 \n    6.0 \n    64.0 \n    33.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    8.0 \n    0.0 \n    0.0 \n    1443.0 \n    2103.0 \n    354678.0 \n  \n  \n    2020-02-01 \n    39472637.0 \n    5314.1 \n    26588.1 \n    6209.6 \n    694.7 \n    666.1 \n    3933.2 \n    176921.0 \n    124343.0 \n    1123010.0 \n    1003380.0 \n    12065.0 \n    106456.0 \n    493798.0 \n    2715.0 \n    231398.0 \n    1321706.0 \n    25936.0 \n    130066.0 \n    73180.0 \n    34683.0 \n    71122.0 \n    98183.0 \n    724391.0 \n    2869.0 \n    14917.0 \n    0.0 \n    25341.0 \n    58527.0 \n    4914.0 \n    885.0 \n    7184.0 \n    3840.0 \n    61038.0 \n    8328.0 \n    242796.0 \n    2180859.0 \n    4985932.0 \n    759531.0 \n    1371864.0 \n    14314.0 \n    196936.0 \n    1263.0 \n    570763.0 \n    2427079.0 \n    226736.0 \n    1927140.0 \n    3647659.0 \n    2437.0 \n    3978903.0 \n    66.0 \n    19551.0 \n    601098.0 \n    109.0 \n    271.0 \n    7837.0 \n    9415.0 \n    1087.0 \n    1217.0 \n    714.0 \n    100408.0 \n    3582.0 \n    180975.0 \n    192.0 \n    13579.0 \n    1012134.0 \n    673261.0 \n    0.0 \n    1134202.0 \n    145.0 \n    0.0 \n    200223.0 \n    9634.0 \n    4398257.0 \n    3057.0 \n    331188.0 \n    7339.0 \n    886.0 \n    2353.0 \n    240158.0 \n    132.0 \n    2485.0 \n    543.0 \n    135.0 \n    43018.0 \n    366.0 \n    37085.0 \n    0.0 \n    0.0 \n    0.0 \n    8087.0 \n    179.0 \n    23.0 \n    1.0 \n    27.0 \n    0.0 \n    29091.0 \n    500418.0 \n    119.0 \n    94.0 \n    63.0 \n    71785.0 \n    121520.0 \n    0.0 \n    0.0 \n    87.0 \n    1.0 \n    42.0 \n    23.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n    2.0 \n    1.0 \n    0.0 \n    848.0 \n    152797.0 \n  \n  \n    2020-03-01 \n    40433029.0 \n    5910.8 \n    26783.6 \n    6333.3 \n    845.9 \n    559.4 \n    4178.2 \n    136150.0 \n    54577.0 \n    1233320.0 \n    1115034.0 \n    12418.0 \n    100595.0 \n    511874.0 \n    5787.0 \n    296540.0 \n    1136172.0 \n    23454.0 \n    130455.0 \n    74845.0 \n    45871.0 \n    69037.0 \n    220796.0 \n    832311.0 \n    2430.0 \n    20391.0 \n    0.0 \n    35228.0 \n    53397.0 \n    4441.0 \n    1861.0 \n    8567.0 \n    6647.0 \n    54120.0 \n    11601.0 \n    70923.0 \n    2093275.0 \n    4295365.0 \n    758664.0 \n    1087792.0 \n    32206.0 \n    162348.0 \n    1134.0 \n    563463.0 \n    2606394.0 \n    364163.0 \n    2006546.0 \n    3852722.0 \n    486.0 \n    5979894.0 \n    6.0 \n    27294.0 \n    703494.0 \n    28.0 \n    347.0 \n    7095.0 \n    6493.0 \n    2924.0 \n    766.0 \n    832.0 \n    79818.0 \n    3326.0 \n    118280.0 \n    191.0 \n    26038.0 \n    655950.0 \n    435911.0 \n    1.0 \n    635944.0 \n    240.0 \n    0.0 \n    228461.0 \n    19178.0 \n    5001588.0 \n    16477.0 \n    310852.0 \n    6774.0 \n    4391.0 \n    4232.0 \n    266785.0 \n    190.0 \n    1935.0 \n    1136.0 \n    91.0 \n    30.0 \n    241.0 \n    8.0 \n    0.0 \n    0.0 \n    4.0 \n    8961.0 \n    260.0 \n    16.0 \n    4.0 \n    77.0 \n    1.0 \n    39133.0 \n    743553.0 \n    549.0 \n    33.0 \n    5.0 \n    100199.0 \n    1323.0 \n    1.0 \n    0.0 \n    0.0 \n    4.0 \n    176.0 \n    23.0 \n    0.0 \n    0.0 \n    5.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n    346.0 \n    112905.0 \n  \n  \n    2020-04-01 \n    35878828.0 \n    5183.5 \n    24534.5 \n    5150.6 \n    637.6 \n    372.6 \n    3662.7 \n    126970.0 \n    62488.0 \n    912836.0 \n    1160767.0 \n    15506.0 \n    116897.0 \n    347814.0 \n    44095.0 \n    269347.0 \n    849831.0 \n    33372.0 \n    83269.0 \n    95240.0 \n    34650.0 \n    50807.0 \n    129160.0 \n    482705.0 \n    3329.0 \n    23609.0 \n    0.0 \n    30516.0 \n    49027.0 \n    2888.0 \n    1425.0 \n    9946.0 \n    5607.0 \n    71541.0 \n    12895.0 \n    96185.0 \n    1735603.0 \n    3387367.0 \n    600246.0 \n    1983188.0 \n    9852.0 \n    77181.0 \n    409.0 \n    832510.0 \n    2168406.0 \n    302865.0 \n    1815269.0 \n    3910062.0 \n    1222.0 \n    5583246.0 \n    16.0 \n    3979.0 \n    276158.0 \n    0.0 \n    70.0 \n    4851.0 \n    8402.0 \n    26190.0 \n    1092.0 \n    1324.0 \n    48218.0 \n    1144.0 \n    75654.0 \n    91.0 \n    5321.0 \n    394261.0 \n    432733.0 \n    0.0 \n    526619.0 \n    87.0 \n    0.0 \n    248425.0 \n    13656.0 \n    4408653.0 \n    6489.0 \n    237606.0 \n    4545.0 \n    2934.0 \n    1581.0 \n    225541.0 \n    4.0 \n    2908.0 \n    1991.0 \n    27.0 \n    18.0 \n    1549.0 \n    31.0 \n    0.0 \n    10.0 \n    17.0 \n    447.0 \n    252.0 \n    31.0 \n    0.0 \n    52.0 \n    2.0 \n    26879.0 \n    480764.0 \n    106.0 \n    0.0 \n    0.0 \n    70052.0 \n    86087.0 \n    4.0 \n    20.0 \n    0.0 \n    1.0 \n    6.0 \n    5.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    2366.0 \n    99266.0 \n  \n  \n    2020-05-01 \n    31458238.0 \n    4259.0 \n    21718.9 \n    4629.0 \n    441.8 \n    409.6 \n    2910.1 \n    77945.0 \n    65305.0 \n    690063.0 \n    892172.0 \n    11181.0 \n    93891.0 \n    295412.0 \n    3436.0 \n    255200.0 \n    1104023.0 \n    28262.0 \n    111422.0 \n    55455.0 \n    31694.0 \n    66717.0 \n    116142.0 \n    446190.0 \n    2349.0 \n    15044.0 \n    0.0 \n    21876.0 \n    35833.0 \n    1931.0 \n    1185.0 \n    10014.0 \n    8059.0 \n    60816.0 \n    7101.0 \n    49763.0 \n    1226045.0 \n    4002175.0 \n    733533.0 \n    995812.0 \n    7736.0 \n    661600.0 \n    311.0 \n    337519.0 \n    1785072.0 \n    170672.0 \n    1285241.0 \n    3711610.0 \n    592.0 \n    4843684.0 \n    15.0 \n    3725.0 \n    436296.0 \n    0.0 \n    5.0 \n    3489.0 \n    9588.0 \n    12630.0 \n    3772.0 \n    293.0 \n    60180.0 \n    1171.0 \n    159144.0 \n    214.0 \n    2214.0 \n    263401.0 \n    425003.0 \n    0.0 \n    453294.0 \n    306.0 \n    0.0 \n    148801.0 \n    8257.0 \n    3554966.0 \n    5059.0 \n    288817.0 \n    4981.0 \n    8573.0 \n    1565.0 \n    213546.0 \n    47.0 \n    981.0 \n    1334.0 \n    87.0 \n    26.0 \n    1300.0 \n    1.0 \n    0.0 \n    9.0 \n    0.0 \n    1487.0 \n    277.0 \n    74.0 \n    0.0 \n    41.0 \n    3.0 \n    19064.0 \n    328456.0 \n    30.0 \n    0.0 \n    4.0 \n    91166.0 \n    21622.0 \n    39.0 \n    0.0 \n    0.0 \n    0.0 \n    16.0 \n    28.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n    2.0 \n    0.0 \n    0.0 \n    714.0 \n    95398.0 \n  \n  \n    2020-06-01 \n    35120892.0 \n    4686.2 \n    24779.3 \n    4960.7 \n    456.4 \n    238.2 \n    3422.9 \n    73457.0 \n    41203.0 \n    895505.0 \n    978806.0 \n    11839.0 \n    111688.0 \n    442728.0 \n    23619.0 \n    273072.0 \n    739542.0 \n    40270.0 \n    90908.0 \n    62225.0 \n    40693.0 \n    94930.0 \n    125208.0 \n    560427.0 \n    1402.0 \n    26047.0 \n    0.0 \n    29820.0 \n    40937.0 \n    4345.0 \n    2004.0 \n    10000.0 \n    4166.0 \n    69463.0 \n    6745.0 \n    36456.0 \n    1337827.0 \n    5247898.0 \n    889759.0 \n    801372.0 \n    8355.0 \n    617967.0 \n    13475.0 \n    312667.0 \n    1754589.0 \n    536137.0 \n    1506803.0 \n    4333513.0 \n    449.0 \n    5288235.0 \n    72.0 \n    12128.0 \n    695070.0 \n    1.0 \n    39.0 \n    3077.0 \n    5687.0 \n    41364.0 \n    2825.0 \n    655.0 \n    70446.0 \n    1829.0 \n    29340.0 \n    145.0 \n    2561.0 \n    292131.0 \n    405429.0 \n    1.0 \n    318246.0 \n    557.0 \n    0.0 \n    178313.0 \n    7998.0 \n    3825437.0 \n    4426.0 \n    418057.0 \n    6004.0 \n    1468.0 \n    2190.0 \n    221919.0 \n    169.0 \n    1455.0 \n    1849.0 \n    34.0 \n    49.0 \n    162.0 \n    20.0 \n    0.0 \n    38.0 \n    14.0 \n    1020.0 \n    194.0 \n    110.0 \n    0.0 \n    609.0 \n    1.0 \n    15341.0 \n    330132.0 \n    118.0 \n    0.0 \n    187.0 \n    73899.0 \n    51298.0 \n    34.0 \n    37.0 \n    89.0 \n    0.0 \n    7.0 \n    9.0 \n    0.0 \n    0.0 \n    0.0 \n    29.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    1257.0 \n    138743.0 \n  \n\n\n\n\n\n\n\n2.2 Data Frame Pivot\n\nimports_filtered <- imports %>% # Select columns that contain \"Data\" or \"2020\", \"2021\", or \"2022\"\n  select(matches(\"Data|202[0-2]\"))\n\nimports_pivot <- imports_filtered %>% # Reshape the data by converting columns to rows\n  pivot_longer(\n    cols = !\"Data Series\", # Exclude the \"Data Series\" column from reshaping\n    names_to = \"timestamp\", # Set the column containing the old column names as \"timestamp\" in the new long format\n    values_to = \"value\" # Set the values in the old columns as \"value\" in the new long format\n  ) %>%\n  mutate(timestamp = lubridate::ymd(paste0(timestamp, \"01\"))) %>% # Convert \"timestamp\" column to a standard date format\n  arrange(timestamp) # Sort the data frame by \"timestamp\" column in ascending order\n\n\nexports_filtered <- exports %>% # Select columns that contain \"Data\" or \"2020\", \"2021\", or \"2022\"\n  select(matches(\"Data|202[0-2]\"))\n\nexports_pivot <- imports_filtered %>% # Reshape the data by converting columns to rows\n  pivot_longer(\n    cols = !\"Data Series\", # Exclude the \"Data Series\" column from reshaping\n    names_to = \"timestamp\", # Set the column containing the old column names as \"timestamp\" in the new long format\n    values_to = \"value\" # Set the values in the old columns as \"value\" in the new long format\n  ) %>%\n  mutate(timestamp = lubridate::ymd(paste0(timestamp, \"01\"))) %>% # Convert \"timestamp\" column to a standard date format\n  arrange(timestamp) # Sort the data frame by \"timestamp\" column in ascending order\n\n\nkable(head(exports_pivot))\n\n\n\n \n  \n    Data Series \n    timestamp \n    value \n  \n \n\n  \n    Total Merchandise Imports (Thousand Dollars) \n    2020-01-01 \n    41180224.0 \n  \n  \n    America (Million Dollars) \n    2020-01-01 \n    5844.1 \n  \n  \n    Asia (Million Dollars) \n    2020-01-01 \n    27128.1 \n  \n  \n    Europe (Million Dollars) \n    2020-01-01 \n    6859.7 \n  \n  \n    Oceania (Million Dollars) \n    2020-01-01 \n    819.7 \n  \n  \n    Africa (Million Dollars) \n    2020-01-01 \n    528.6 \n  \n\n\n\n\n\nin this part, we applied R to form pivet table,as we can see there will be 3 columns, represent different value to each continent。"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#horizon-plot-average-import-prices-from-different-countries-jan-2020-to-dec-2022",
    "href": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#horizon-plot-average-import-prices-from-different-countries-jan-2020-to-dec-2022",
    "title": "TAKEHOME04 Putting Visual Analytics into Practical Use",
    "section": "2.1 Horizon Plot: Average Import Prices from Different Countries (Jan 2020 to Dec 2022)",
    "text": "2.1 Horizon Plot: Average Import Prices from Different Countries (Jan 2020 to Dec 2022)\n\nimports_pivot\n\n# A tibble: 4,284 × 3\n   `Data Series`                                timestamp      value\n   <chr>                                        <date>         <dbl>\n 1 Total Merchandise Imports (Thousand Dollars) 2020-01-01 41180224 \n 2 America (Million Dollars)                    2020-01-01     5844.\n 3 Asia (Million Dollars)                       2020-01-01    27128.\n 4 Europe (Million Dollars)                     2020-01-01     6860.\n 5 Oceania (Million Dollars)                    2020-01-01      820.\n 6 Africa (Million Dollars)                     2020-01-01      529.\n 7 European Union (Million Dollars)             2020-01-01     4371.\n 8 Belgium (Thousand Dollars)                   2020-01-01   124193 \n 9 Denmark (Thousand Dollars)                   2020-01-01    84860 \n10 France (Thousand Dollars)                    2020-01-01  1491370 \n# … with 4,274 more rows\n\n\n\nimports_pivot %>% \n  ggplot() +\n  geom_horizon(aes(x = timestamp, y=value), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Data Series`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Import Prices from Different Countries (Jan 2020 to Dec 2022)')\n\n\n\n\nFrom this import average price from different region from 2020 to 2022,(oringin set midpoint, which means red represent below midpoint, otherwise is blue) .The overall trend is the area of blue color is growing gradually.in the left side of the graph, most of the area is red, very limited is blue.This means the import price is growing with the covid ending."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#total-import-and-export-by-continents",
    "href": "TAKEHOME/TAKEHOME04/TAKEHOME04.html#total-import-and-export-by-continents",
    "title": "TAKEHOME04 Putting Visual Analytics into Practical Use",
    "section": "2.2 Total Import and Export by Continents",
    "text": "2.2 Total Import and Export by Continents\n\nplot_ly(imports_transposed, x = ~timestamp, y = ~`Asia (Million Dollars)`, \n        type = \"scatter\", mode = \"lines\", name=\"Asia\", \n        hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Europe (Million Dollars)`, \n            name=\"EU\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`America (Million Dollars)`, \n            name=\"America\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Oceania (Million Dollars)`, \n            name=\"Oceania\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Africa (Million Dollars)`, \n            name=\"Africa\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  layout(title = \"Total Imports by Continents over Time\",\n         xaxis = list(title = \"Date\",\n                      tickformat = \"%Y-%m\",\n                      tickmode = \"auto\",\n                      nticks = 10),\n         yaxis = list(title = \"Total Imports (Millons)\"))\n\n\n\n\n\nIn this graph, we plot line to represent the trend of total imports value during covid19 grouped by continents.\nGenerally speaking, Asia is the highest all the time, while Africa almost take the lowest all the way, although some time exceed Oceania. Other 3 take the middle seperately.\nExcept for Afica, all of the continents’ import value growed fluctuatly, as the growth of EU, America,Oceania are not obvious because of some main peak and drop.\nFrom time series,there are some main low points :2020-07, 2021-07.due to the specific economic and social circumstances, it seems that over a round, the trend will repeat.\nthere are some main high points too :2021-03, 2022-01.Due to the specific economic and social circumstances, it seems that over a round, the trend will repeat too.\n\nplot_ly(exports_transposed, x = ~timestamp, y = ~`Asia (Million Dollars)`, \n        type = \"scatter\", mode = \"lines\", name=\"Asia\", \n        hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Europe (Million Dollars)`, \n            name=\"EU\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`America (Million Dollars)`, \n            name=\"America\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Oceania (Million Dollars)`, \n            name=\"Oceania\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  add_lines(x=~timestamp, y=~`Africa (Million Dollars)`, \n            name=\"Africa\", hovertemplate = \"%{x}, %{y:.2f} Millon\") %>%\n  layout(title = \"Total Exports by Continents over Time\",\n         xaxis = list(title = \"Date\",\n                      tickformat = \"%Y-%m\",\n                      tickmode = \"auto\",\n                      nticks = 10),\n         yaxis = list(title = \"Total Exports (Millons)\"))\n\n\n\n\n\nLet’s look at the trend of total emports value during covid19 grouped by continents.\nGenerally speaking, Asia is the highest all the time, while Africa took the lowest all the way. although, America is higher than EU in most of time, they are fluctuating and catching up consistently.\nFrom time series,there are some main low points :2021-01,2022-05.Due to the specific economic and social circumstances, it seems that over a round, the trend will repeat.\nThere are some main high points too :2020-07, 2021-02.Due to the specific economic and social circumstances, it seems that over a round, the trend will repeat too."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html",
    "title": "Takehome03",
    "section": "",
    "text": "pacman::p_load(dplyr, tidyverse, DT, ggplot2, ggiraph, \n               ggstatsplot, patchwork, plotly, gganimate, \n               ggthemes, corrplot, heatmaply,lubridate,scales,ggrepel,ggpubr)\n\n\nresale_data <- read_csv(\"data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n\nDT::datatable(head(resale_data,20)) \n\n\n\n\n\n\nConvert the date format from YYYY-MM to Y-M-D:\n\nresale_data <- resale_data %>% \n  mutate(month = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\"))\n\nConvert the date format to Y-M-D, D is first day of each month:\n\nresale_345room_2022 <- resale_data %>% \n  filter(flat_type %in% c(\"3 ROOM\", \"4 ROOM\", \"5 ROOM\"), \n         year(month) == '2022') %>%\n  mutate(\"unit_price_sqm\" = resale_price/floor_area_sqm)\n\n\nDT::datatable(head(resale_345room_2022))"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#average-resale-price-by-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#average-resale-price-by-type",
    "title": "Takehome03",
    "section": "2.1 Average Resale Price By Type",
    "text": "2.1 Average Resale Price By Type\n\n# Group data by flat type\nresale_345room_2022_grouped <- resale_345room_2022 %>%\n  group_by(flat_type, month) %>%\n  summarise(resale_price = mean(resale_price)/1000)\n\n# Plot monthly trend by flat type\nggplot(resale_345room_2022_grouped, aes(x = month, y = resale_price, color = flat_type)) +\n  geom_line() +\n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  labs(x = \"Month\", y = \"Resale Price (SGD, thousands)\", color = \"Flat Type\",title = \"Monthly Resale Price Trend by Flat Type (2022)\") +\n  theme_bw()\n\n\n\n\nFrom this graph, the average resale price of 3 types, 5 room is the highest, then is the 4-room, the price of 3-room is the lowest.\nThe price is rising while the area rises."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#average-resale-per-sqm-price-by-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#average-resale-per-sqm-price-by-type",
    "title": "Takehome03",
    "section": "2.2Average Resale Per SQM Price By Type",
    "text": "2.2Average Resale Per SQM Price By Type\n\n# Group the data by flat type and month, and calculate the average unit resale price\nresale_avg <- resale_345room_2022 %>%\n  group_by(flat_type, month) %>%\n  summarise(avg_resale_price = mean(resale_price)/mean(floor_area_sqm))\n\n# Plot the line chart with dynamic tooltips using ggplot and plotly\np <- ggplot(resale_avg, aes(x = month, y = avg_resale_price, color = flat_type, group = flat_type)) +\n  geom_line() +\n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  scale_y_continuous(labels = function(x) paste0(x/1000, \"k\")) +\n  labs(x = \"Month\", y = \"Unit Price\", title = \"Average Unit Resale Price Trend by Flat Type (2022)\") +\n  theme_bw()\n\nggplotly(p, tooltip = c(\"month\", \"avg_resale_price\"))\n\n\n\n\n\nFrom this graph, the unit resale price of 3 types, 4 room is the overall highest, then is the 3-room, the price of 5-room is the lowest.\nFrom the time view, all of the 3 types grow gradually while there are some disrupted peak and drop especially around March,Auf and Nov.\nFor the 4 ROOM type,it grew smoothly before July, then a suddenly drop in Aug. after that increased rapidly while drop again in Nov.\nFor the 5 ROOM type,it grew fluctuately and didn’t grow as much as other 2 types eventually. It rise and decrease alternatively every month, the sudden drop happened between 2 stages of increase May to July, Aug to Oct.\nFor the 4 ROOM type,it grew fluctuately but saw a definite growth compared with the start of the year. There are 3 disruptive rises happend in March, June, September respectively.\nThe price is rising while the area rises."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Takehome03",
    "section": "2.3 Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "2.3 Visualizing the uncertainty of point estimates: ggplot2 methods\n\nmy_sum <- resale_345room_2022 %>%\n  group_by(flat_type) %>%\n  summarise(\n    n=n(),\n    mean=mean(unit_price_sqm),\n    sd=sd(unit_price_sqm)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=flat_type, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=flat_type, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          unit price by flat type\")"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#the-distribution-of-lease-commence-date-of-2022-by-flat-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#the-distribution-of-lease-commence-date-of-2022-by-flat-type",
    "title": "Takehome03",
    "section": "2.4 The Distribution of Lease Commence Date of 2022 by Flat Type",
    "text": "2.4 The Distribution of Lease Commence Date of 2022 by Flat Type\n\nresale_3room <- resale_345room_2022 %>%  \n  filter(flat_type == \"3 ROOM\")\n         \ngghistostats(\n  data = resale_3room ,\n  x = lease_commence_date,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"lease commence date\",\n  title=\"The Distribution of Lease Commence Date for 3-Room (2022)\",\n)\n\n\n\n\n\nresale_4room <- resale_345room_2022  %>% \n  filter(flat_type == \"4 ROOM\")\n         \ngghistostats(\n  data = resale_4room ,\n  x = lease_commence_date,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"lease commence date\",\n  title=\"The Distribution of Lease Commence Date for 4-Room (2022)\",\n)\n\n\n\n\n\nresale_5room <- resale_345room_2022  %>%\n  filter(flat_type == \"5 ROOM\")\n         \ngghistostats(\n  data = resale_5room ,\n  x = lease_commence_date,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"lease commence date\",\n  title=\"The Distribution of Lease Commence Date for 5-Room (2022)\",\n)"
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#one-way-anova-test-on-unit-price-sqm-by-flat-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#one-way-anova-test-on-unit-price-sqm-by-flat-type",
    "title": "Takehome03",
    "section": "2.5 One-way ANOVA test on Unit Price / sqm by Flat Type",
    "text": "2.5 One-way ANOVA test on Unit Price / sqm by Flat Type\n\np <- ggbetweenstats(\n  data = resale_345room_2022,\n  x = flat_type, \n  y = unit_price_sqm,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\np + labs(title = \"One-way ANOVA test on Unit Price / sqm by Flat Type\", \n         y = \"Unit Price / sqm\",\n         x = \"Flat Type\")\n\n\n\n\nFor the mean of unit price of 3 types flats, 4 room is the highest, then is the 3 room, then is the 5 room\nFor the max of unit price of 3 types flats,4 room is the highest,while 3 and 5 is almost the same.\nFor the width of unit price of 3 types flats,3 room is the wildest,then is the 5 room, last is the 4 room."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#resale-price-by-town-and-flat-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#resale-price-by-town-and-flat-type",
    "title": "Takehome03",
    "section": "2.6 Resale Price by Town and Flat Type",
    "text": "2.6 Resale Price by Town and Flat Type\n\nggplot(resale_345room_2022, aes(x=town, y=resale_price/1000, fill=flat_type)) + \n  geom_boxplot() + \n  xlab(\"Town\") + \n  ylab(\"Resale Price (in '000s')\") + \n  scale_fill_manual(values=c(\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"pink\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), \n        plot.margin = unit(c(1, 5, 1, 1), \"cm\"), \n        axis.title.x = element_text(size = 18), \n        axis.title.y = element_text(size = 18),\n        legend.title = element_blank(),\n        legend.text = element_text(size = 18)) +\n  labs(title = \"Resale Price by Town and Flat Type\") +\n  scale_y_continuous(labels = function(x) paste0(x, \"k\")) +\n  coord_cartesian(clip = \"off\", ylim = c(0, 1400)) +\n  theme(plot.title = element_text(size = 18, face = \"bold\"),\n        axis.text.y = element_text(size = 18),\n        legend.position = \"bottom\",\n        legend.box = \"horizontal\",\n        legend.margin = margin(t = 0, r = 0, b = 0, l = 0)\n        ) \n\n\n\n\nFor the average price range of 3 room type (75% - 25%), 5-room seems has a biggest range within different districts, which in graph shows the box is longer.\nwhile 3-room seems has a smallest range within different districts which in graph shows the box is shorter."
  },
  {
    "objectID": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#from-average-resale-per-sqm-price-by-type",
    "href": "TAKEHOME/TAKEHOME03/TAKEHOME03.html#from-average-resale-per-sqm-price-by-type",
    "title": "Takehome03",
    "section": "From Average Resale Per SQM Price By Type",
    "text": "From Average Resale Per SQM Price By Type\n4- ROOM is the highest, this may related to customer’s preference and population structure in Singapore property market.\nIn Aug and December, there is a big drop on average resale per sqm price of all 3 types, we can find the root cause and come up with some action.\nIn March, there will be a obview fluctuation on average resale per sqm price of all 3 types, we can find the root cause and come up with some action."
  }
]